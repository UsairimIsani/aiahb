{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport cv2\nfrom tqdm import tqdm\nfrom glob import glob\nfrom PIL import Image\nfrom skimage.transform import resize\nfrom sklearn.model_selection import train_test_split, KFold\n\nimport tensorflow as tf\nimport tensorflow.keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nK.set_image_data_format('channels_last')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"V000z_TVFFnk","execution":{"iopub.status.busy":"2022-04-14T01:07:01.786330Z","iopub.execute_input":"2022-04-14T01:07:01.786791Z","iopub.status.idle":"2022-04-14T01:07:08.136099Z","shell.execute_reply.started":"2022-04-14T01:07:01.786714Z","shell.execute_reply":"2022-04-14T01:07:08.135331Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())","metadata":{"execution":{"iopub.status.busy":"2022-04-14T01:07:08.137662Z","iopub.execute_input":"2022-04-14T01:07:08.137922Z","iopub.status.idle":"2022-04-14T01:07:10.211835Z","shell.execute_reply.started":"2022-04-14T01:07:08.137885Z","shell.execute_reply":"2022-04-14T01:07:10.210722Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Building the training dataset.\nLet's look at the train image list","metadata":{"id":"H36EHCoUFFoL"}},{"cell_type":"code","source":"path = \"../input/ultrasound-nerve-segmentation/train/\"\nfile_list = os.listdir(path)\nfile_list[:20]","metadata":{"_kg_hide-output":false,"id":"5IQvMMbEFFoO","outputId":"93ea183a-7ae8-45fc-967e-c0f05d965c00","execution":{"iopub.status.busy":"2022-04-14T01:07:10.212774Z","iopub.execute_input":"2022-04-14T01:07:10.213029Z","iopub.status.idle":"2022-04-14T01:07:10.528479Z","shell.execute_reply.started":"2022-04-14T01:07:10.212993Z","shell.execute_reply":"2022-04-14T01:07:10.527764Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"**Sort the file list in ascending order and seperate it into images and masks**<br/>\nEach file has the form of either \"subject_imageNum.tif\" or \"subject_imageNum_mask.tif\", so we can extract `subject` and `imageNum` from each file name by using regular expression. `\"[0-9]+\"` means to find the first consecutive number.<br/>","metadata":{"id":"w0rkSYxsFFoc"}},{"cell_type":"code","source":"train_image = []\ntrain_mask = glob(path + '*_mask*')\n\nfor i in train_mask:\n    train_image.append(i.replace('_mask', ''))\n        \nprint(train_image[:10],\"\\n\" ,train_mask[:10])","metadata":{"id":"UiEiHaJHFFor","outputId":"abfe2f7f-5c63-4dd6-d0d3-52329554f921","execution":{"iopub.status.busy":"2022-04-14T01:07:10.530605Z","iopub.execute_input":"2022-04-14T01:07:10.530873Z","iopub.status.idle":"2022-04-14T01:07:10.567541Z","shell.execute_reply.started":"2022-04-14T01:07:10.530839Z","shell.execute_reply":"2022-04-14T01:07:10.566887Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Display the first image and mask of the first subject.\nimage1 = np.array(Image.open(path+\"1_1.tif\"))\nimage1_mask = np.array(Image.open(path+\"1_1_mask.tif\"))\nimage1_mask = np.ma.masked_where(image1_mask == 0, image1_mask)\n\nfig, ax = plt.subplots(1,3,figsize = (16,12))\nax[0].imshow(image1, cmap = 'gray')\n\nax[1].imshow(image1_mask, cmap = 'gray')\n\nax[2].imshow(image1, cmap = 'gray', interpolation = 'none')\nax[2].imshow(image1_mask, cmap = 'jet', interpolation = 'none', alpha = 0.7)","metadata":{"id":"hjv-upiCFFo3","outputId":"2c95cff6-f644-4bbc-8b71-c720e964dacc","execution":{"iopub.status.busy":"2022-04-14T01:07:10.568746Z","iopub.execute_input":"2022-04-14T01:07:10.569151Z","iopub.status.idle":"2022-04-14T01:07:11.147070Z","shell.execute_reply.started":"2022-04-14T01:07:10.569113Z","shell.execute_reply":"2022-04-14T01:07:11.141940Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Now, I try to load all image files and store them variables X and y. Afther doing this, I recognize that it takes very much memory.<br/>\nPlease let me know if there are several efficient ways to store image file","metadata":{"id":"fxk0ICSrFFpD"}},{"cell_type":"markdown","source":"## How to deal with train_masks.csv ?","metadata":{"id":"31B8z8M9FFpF"}},{"cell_type":"code","source":"width = 128\nheight = 128","metadata":{"id":"YPvr7aOOFFpQ","execution":{"iopub.status.busy":"2022-04-14T01:07:11.149769Z","iopub.execute_input":"2022-04-14T01:07:11.150476Z","iopub.status.idle":"2022-04-14T01:07:11.154611Z","shell.execute_reply.started":"2022-04-14T01:07:11.150423Z","shell.execute_reply":"2022-04-14T01:07:11.153745Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Let's check that I did well","metadata":{"id":"FQhxK5L0FFps"}},{"cell_type":"markdown","source":"Let's modularize this work.","metadata":{"id":"F21ZRTvPFFpu"}},{"cell_type":"code","source":"from tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.layers import Input, Activation, BatchNormalization, Dropout, Lambda, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate,add\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint","metadata":{"id":"Gpbv3Zc4FFqi","execution":{"iopub.status.busy":"2022-04-14T01:07:11.155899Z","iopub.execute_input":"2022-04-14T01:07:11.156387Z","iopub.status.idle":"2022-04-14T01:07:11.168872Z","shell.execute_reply.started":"2022-04-14T01:07:11.156346Z","shell.execute_reply":"2022-04-14T01:07:11.167768Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def dice_coef(y_true, y_pred):\n    smooth = 0.0\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\ndef jacard(y_true, y_pred):\n\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum ( y_true_f * y_pred_f)\n    union = K.sum ( y_true_f + y_pred_f - y_true_f * y_pred_f)\n\n    return intersection/union\n\n\ndef dice_coef_loss(y_true, y_pred):\n    return -dice_coef(y_true, y_pred)\n","metadata":{"id":"xyAgRz_tFFqp","execution":{"iopub.status.busy":"2022-04-14T01:07:11.170591Z","iopub.execute_input":"2022-04-14T01:07:11.171304Z","iopub.status.idle":"2022-04-14T01:07:11.181667Z","shell.execute_reply.started":"2022-04-14T01:07:11.171264Z","shell.execute_reply":"2022-04-14T01:07:11.180864Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"pos_mask = []\npos_img = []\nneg_mask = []\nneg_img = []\n\nfor mask_path, img_path in zip(train_mask, train_image):\n    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n    if np.sum(mask) == 0:\n        neg_mask.append(mask_path)\n        neg_img.append(img_path)\n    else:\n        pos_mask.append(mask_path)\n        pos_img.append(img_path)","metadata":{"id":"zO4WD60TQ_HL","execution":{"iopub.status.busy":"2022-04-14T01:07:11.183176Z","iopub.execute_input":"2022-04-14T01:07:11.183903Z","iopub.status.idle":"2022-04-14T01:07:45.372948Z","shell.execute_reply.started":"2022-04-14T01:07:11.183624Z","shell.execute_reply":"2022-04-14T01:07:45.372167Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"!mkdir generated\n!mkdir generated/img","metadata":{"id":"YGNJWh6ZQ_HW","execution":{"iopub.status.busy":"2022-04-14T01:07:45.376223Z","iopub.execute_input":"2022-04-14T01:07:45.376878Z","iopub.status.idle":"2022-04-14T01:07:47.009033Z","shell.execute_reply.started":"2022-04-14T01:07:45.376801Z","shell.execute_reply":"2022-04-14T01:07:47.008057Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def flip_up_down(img):\n    newImg = img.copy()\n    return cv2.flip(newImg, 0)\n\ndef flip_right_left(img):\n    newImg = img.copy()\n    return cv2.flip(newImg, 1)","metadata":{"id":"_vN5uEKUjzeK","execution":{"iopub.status.busy":"2022-04-14T01:07:47.010759Z","iopub.execute_input":"2022-04-14T01:07:47.011055Z","iopub.status.idle":"2022-04-14T01:07:47.017107Z","shell.execute_reply.started":"2022-04-14T01:07:47.011019Z","shell.execute_reply":"2022-04-14T01:07:47.015849Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"gen_img = []\ngen_mask = []\n\nfor (img_path, mask_path) in tqdm(zip(pos_img, pos_mask)):\n    image_name = img_path.split('/')[-1].split('.')[0]\n\n    uf_img_path = 'generated/img/'+image_name+'_uf.jpg'\n    uf_mask_path = 'generated/img/'+image_name+'_uf_mask.jpg'\n    rf_img_path = 'generated/img/'+image_name+'_rf.jpg'\n    rf_mask_path = 'generated/img/'+image_name+'_rf_mask.jpg'\n\n    img = cv2.imread(img_path)\n    mask = cv2.imread(mask_path)\n\n    uf_img = flip_up_down(img)\n    uf_mask = flip_up_down(mask)\n    cv2.imwrite(uf_img_path, uf_img)\n    cv2.imwrite(uf_mask_path, uf_mask)\n    \n    rf_img = flip_right_left(img)\n    rf_mask = flip_right_left(mask)\n    cv2.imwrite(rf_img_path, rf_img)\n    cv2.imwrite(rf_mask_path, rf_mask)\n    \n    gen_img.append(uf_img_path)\n    gen_mask.append(uf_mask_path)\n    gen_img.append(rf_img_path)\n    gen_mask.append(rf_mask_path)","metadata":{"id":"bL-tRF1DhR-J","outputId":"0554e1a6-4f09-443a-9c3c-310a40020d80","execution":{"iopub.status.busy":"2022-04-14T01:07:47.018528Z","iopub.execute_input":"2022-04-14T01:07:47.019033Z","iopub.status.idle":"2022-04-14T01:08:58.491987Z","shell.execute_reply.started":"2022-04-14T01:07:47.018995Z","shell.execute_reply":"2022-04-14T01:08:58.490718Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"aug_img = gen_img + train_image\naug_mask = gen_mask + train_mask\n\ndf_ = pd.DataFrame(data={\"filename\": aug_img, 'mask' : aug_mask})\ndf = df_.sample(frac=1).reset_index(drop=True)\n\nkf = KFold(n_splits = 5, shuffle=False)","metadata":{"id":"dXlzh9nwFFq_","execution":{"iopub.status.busy":"2022-04-14T01:08:58.493391Z","iopub.execute_input":"2022-04-14T01:08:58.493651Z","iopub.status.idle":"2022-04-14T01:08:58.512225Z","shell.execute_reply.started":"2022-04-14T01:08:58.493615Z","shell.execute_reply":"2022-04-14T01:08:58.511418Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.layers import *\n\ndef res_block(inputs,filter_size):\n    \"\"\"\n    res_block -- Residual block for building res path\n    \n    Arguments:\n    inputs {<class 'tensorflow.python.framework.ops.Tensor'>} -- input for residual block\n    filter_size {int} -- convolutional filter size \n    \n    Returns:\n    add {<class 'tensorflow.python.framework.ops.Tensor'>} -- addition of two convolutional filter output  \n    \"\"\"\n    # First Conv2D layer\n    cb1 = Conv2D(filter_size,(3,3),padding = 'same',activation=\"relu\")(inputs)\n    # Second Conv2D layer parallel to the first one\n    cb2 = Conv2D(filter_size,(1,1),padding = 'same',activation=\"relu\")(inputs)\n    # Addition of cb1 and cb2\n    add = Add()([cb1,cb2])\n    \n    return add\n\ndef res_path(inputs,filter_size,path_number):\n    \"\"\"\n    res_path -- residual path / modified skip connection\n    \n    Arguments:\n    inputs {<class 'tensorflow.python.framework.ops.Tensor'>} -- input for res path\n    filter_size {int} -- convolutional filter size \n    path_number {int} -- path identifier \n    \n    Returns:\n    skip_connection {<class 'tensorflow.python.framework.ops.Tensor'>} -- final res path\n    \"\"\"\n    # Minimum one residual block for every res path\n    skip_connection = res_block(inputs, filter_size)\n\n    # Two serial residual blocks for res path 2\n    if path_number == 2:\n        skip_connection = res_block(skip_connection,filter_size)\n    \n    # Three serial residual blocks for res path 1\n    elif path_number == 1:\n        skip_connection = res_block(skip_connection,filter_size)\n        skip_connection = res_block(skip_connection,filter_size)\n    \n    return skip_connection\n\ndef decoder_block(inputs, res, out_channels, depth):\n    \n    \"\"\"\n    decoder_block -- decoder block formation\n    \n    Arguments:\n    inputs {<class 'tensorflow.python.framework.ops.Tensor'>} -- input for decoder block\n    mid_channels {int} -- no. of mid channels \n    out_channels {int} -- no. of out channels\n    \n    Returns:\n    db {<class 'tensorflow.python.framework.ops.Tensor'>} -- returning the decoder block\n    \"\"\"\n    conv_kwargs = dict(\n        activation='relu',\n        padding='same',\n        kernel_initializer='he_normal',\n        data_format='channels_last'  \n    )\n    \n    # UpConvolutional layer\n    db = Conv2DTranspose(out_channels, (2, 2), strides=(2, 2))(inputs)\n    db = concatenate([db, res], axis=3)\n    # First conv2D layer \n    db = Conv2D(out_channels, 3, **conv_kwargs)(db)\n    # Second conv2D layer\n    db = Conv2D(out_channels, 3, **conv_kwargs)(db)\n\n    if depth > 2:\n        # Third conv2D layer\n        db = Conv2D(out_channels, 3, **conv_kwargs)(db)\n\n    return db\n\ndef TransResUNet(input_size=(512, 512, 1)):\n    \"\"\"\n    TransResUNet -- main architecture of TransResUNet\n    \n    Arguments:\n    input_size {tuple} -- size of input image\n    \n    Returns:\n    model {<class 'tensorflow.python.keras.engine.training.Model'>} -- final model\n    \"\"\"\n    \n    # Input \n    inputs = Input(input_size)\n    \n    # Handling input channels \n    # input with 1 channel will be converted to 3 channels to be compatible with VGG16 pretrained encoder \n#     if input_size[-1] < 3:\n#         inp = Conv2D(3, 1)(inputs)                         \n#         input_shape = (input_size[0], input_size[0], 3)  \n#     else:\n#         inp = inputs\n#         input_shape = input_size\n\n    # VGG16 with imagenet weights\n    encoder = VGG16(include_top=False, weights='imagenet', input_shape=input_size)\n       \n    # First encoder block\n    enc1 = encoder.get_layer(name='block1_conv1')(inputs)\n    enc1 = encoder.get_layer(name='block1_conv2')(enc1)\n    enc2 = MaxPooling2D(pool_size=(2, 2))(enc1)\n    \n    # Second encoder block\n    enc2 = encoder.get_layer(name='block2_conv1')(enc2)\n    enc2 = encoder.get_layer(name='block2_conv2')(enc2)\n    enc3 = MaxPooling2D(pool_size=(2, 2))(enc2)\n    \n    # Third encoder block\n    enc3 = encoder.get_layer(name='block3_conv1')(enc3)\n    enc3 = encoder.get_layer(name='block3_conv2')(enc3)\n    enc3 = encoder.get_layer(name='block3_conv3')(enc3)\n    enc4 = MaxPooling2D(pool_size=(2, 2))(enc3)\n    \n    # Fourth encoder block\n    enc4 = encoder.get_layer(name='block4_conv1')(enc4)\n    enc4 = encoder.get_layer(name='block4_conv2')(enc4)\n    enc4 = encoder.get_layer(name='block4_conv3')(enc4)\n    center = MaxPooling2D(pool_size=(2, 2))(enc4)\n\n    # Center block\n    center = Conv2D(1024, (3, 3), activation='relu', padding='same')(center)\n    center = Conv2D(1024, (3, 3), activation='relu', padding='same')(center)\n    \n    # classification branch\n    cls = Conv2D(256, (3,3), activation='relu')(center)\n#     cls = Conv2D(128, (3,3), activation='relu')(cls)\n    cls = Conv2D(1, (1,1))(cls)\n    cls = GlobalAveragePooling2D()(cls)\n    cls = Activation('sigmoid', name='class')(cls)\n    clsr = Reshape((1, 1, 1))(cls)\n    \n    \n    # Decoder block corresponding to fourth encoder\n    res_path4 = res_path(enc4,256,4)\n    dec4 = decoder_block(center, res_path4, 512, 4)\n\n    # Decoder block corresponding to third encoder\n    res_path3 = res_path(enc3,128,3)\n    dec3 = decoder_block(dec4, res_path3, 256, 3)\n    \n    # Decoder block corresponding to second encoder\n    res_path2 = res_path(enc2,64,2)\n    dec2 = decoder_block(dec3, res_path2, 128, 2)\n    \n    # Final Block concatenation with first encoded feature \n    res_path1 = res_path(enc1,32,1)\n    dec1 = decoder_block(dec2, res_path1, 64, 1)\n\n    # Output\n    out = Conv2D(1, 1)(dec1)\n    out = Activation('sigmoid')(out)\n    out = multiply(inputs=[out,clsr], name='seg')\n    \n    # Final model\n    model = Model(inputs=[inputs], outputs=[out, cls])\n    \n    return model","metadata":{"id":"M_d0LZZj3zOz","execution":{"iopub.status.busy":"2022-04-14T01:08:58.513512Z","iopub.execute_input":"2022-04-14T01:08:58.513809Z","iopub.status.idle":"2022-04-14T01:08:58.540336Z","shell.execute_reply.started":"2022-04-14T01:08:58.513766Z","shell.execute_reply":"2022-04-14T01:08:58.539527Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# From: https://github.com/zhixuhao/unet/blob/master/data.py\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\n\ndef train_generator(data_frame, batch_size, train_path, aug_dict,\n        image_color_mode=\"rgb\",\n        mask_color_mode=\"grayscale\",\n        image_save_prefix=\"image\",\n        mask_save_prefix=\"mask\",\n        save_to_dir=None,\n        target_size=(256,256),\n        seed=1):\n    '''\n    can generate image and mask at the same time use the same seed for\n    image_datagen and mask_datagen to ensure the transformation for image\n    and mask is the same if you want to visualize the results of generator,\n    set save_to_dir = \"your path\"\n    '''\n    image_datagen = ImageDataGenerator(**aug_dict)\n    mask_datagen = ImageDataGenerator(**aug_dict)\n    \n    image_generator = image_datagen.flow_from_dataframe(\n        data_frame,\n        directory = train_path,\n        x_col = \"filename\",\n        class_mode = None,\n        color_mode = image_color_mode,\n        target_size = target_size,\n        batch_size = batch_size,\n        save_to_dir = save_to_dir,\n        save_prefix  = image_save_prefix,\n        seed = seed)\n\n    mask_generator = mask_datagen.flow_from_dataframe(\n        data_frame,\n        directory = train_path,\n        x_col = \"mask\",\n        class_mode = None,\n        color_mode = mask_color_mode,\n        target_size = target_size,\n        batch_size = batch_size,\n        save_to_dir = save_to_dir,\n        save_prefix  = mask_save_prefix,\n        seed = seed)\n\n    train_gen = zip(image_generator, mask_generator)\n    \n    for (img, mask) in train_gen:\n        img, mask, label = adjust_data(img, mask)\n        yield (img,[mask,label])\n\ndef adjust_data(img,mask):\n    img = preprocess_input(img)\n    mask = mask / 255\n    mask[mask > 0.5] = 1\n    mask[mask <= 0.5] = 0\n    masks_sum = np.sum(mask, axis=(1,2,3)).reshape((-1, 1))\n    class_lab = (masks_sum != 0) + 0.\n    \n    return (img, mask, class_lab)","metadata":{"id":"IEX0f8GhFFq3","execution":{"iopub.status.busy":"2022-04-14T01:08:58.541732Z","iopub.execute_input":"2022-04-14T01:08:58.542182Z","iopub.status.idle":"2022-04-14T01:08:58.555619Z","shell.execute_reply.started":"2022-04-14T01:08:58.542138Z","shell.execute_reply":"2022-04-14T01:08:58.554958Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"histories = []\nlosses = []\naccuracies = []\ndicecoefs = []\njacards = []\n\ntrain_generator_args = dict(rotation_range=0.2,\n                            width_shift_range=0.05,\n                            height_shift_range=0.05,\n                            shear_range=0.05,\n                            zoom_range=0.05,\n                            horizontal_flip=True,\n                            fill_mode='nearest')\n\nEPOCHS = 40\nBATCH_SIZE = 32\n\nfor k, (train_index, test_index) in enumerate(kf.split(df)):\n    train_data_frame = df.iloc[train_index]\n    test_data_frame = df.iloc[test_index]\n    \n    train_gen = train_generator(train_data_frame, BATCH_SIZE,\n                                None,\n                                train_generator_args,\n                                target_size=(height, width))\n\n    test_gener = train_generator(test_data_frame, BATCH_SIZE,\n                                None,\n                                dict(),\n                                target_size=(height, width))\n\n    model = TransResUNet(input_size=(height,width, 3))\n    model.compile(optimizer=Adam(lr=5e-6), loss={'seg':dice_coef_loss, 'class':'binary_crossentropy'}, \\\n                      loss_weights={'seg':50, 'class':1}, metrics=[jacard, dice_coef, 'binary_accuracy'])\n    model.summary()\n\n    model_checkpoint = ModelCheckpoint(str(k+1) + '_unet_ner_seg.hdf5', \n                                       verbose=1, \n                                       save_best_only=True)\n\n    history = model.fit(train_gen,\n                          steps_per_epoch=len(train_data_frame) // BATCH_SIZE, \n                          epochs=EPOCHS, \n                          callbacks=[model_checkpoint],\n                          validation_data = test_gener,\n                          validation_steps=len(test_data_frame) // BATCH_SIZE)\n    \n    model = load_model(str(k+1) + '_unet_ner_seg.hdf5', custom_objects={'dice_coef_loss': dice_coef_loss, 'jacard': jacard, 'dice_coef': dice_coef})\n    \n    test_gen = train_generator(test_data_frame, BATCH_SIZE,\n                                None,\n                                dict(),\n                                target_size=(height, width))\n    results = model.evaluate(test_gen, steps=len(test_data_frame) // BATCH_SIZE)\n    results = dict(zip(model.metrics_names,results))\n    \n    histories.append(history)\n    accuracies.append(results['seg_binary_accuracy'])\n    losses.append(results['seg_loss'])\n    dicecoefs.append(results['seg_dice_coef'])\n    jacards.append(results['seg_jacard']) \n    \n    break","metadata":{"id":"ZlGRbdQ2FFrG","outputId":"586099a3-815f-4cd1-b5c4-1072bf7d055c","execution":{"iopub.status.busy":"2022-04-14T01:08:58.557011Z","iopub.execute_input":"2022-04-14T01:08:58.557414Z","iopub.status.idle":"2022-04-14T02:33:48.522810Z","shell.execute_reply.started":"2022-04-14T01:08:58.557376Z","shell.execute_reply":"2022-04-14T02:33:48.521768Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nfor h, history in enumerate(histories):\n\n    keys = history.history.keys()\n    fig, axs = plt.subplots(1, len(keys)//2, figsize = (25, 5))\n    fig.suptitle('No. ' + str(h+1) + ' Fold Results', fontsize=30)\n\n    for k, key in enumerate(list(keys)[:len(keys)//2]):\n        training = history.history[key]\n        validation = history.history['val_' + key]\n\n        epoch_count = range(1, len(training) + 1)\n\n        axs[k].plot(epoch_count, training, 'r--')\n        axs[k].plot(epoch_count, validation, 'b-')\n        axs[k].legend(['Training ' + key, 'Validation ' + key])\n                \n    with open(str(h+1) + '_lungs_trainHistoryDict', 'wb') as file_pi:\n        pickle.dump(history.history, file_pi)","metadata":{"id":"JG6XfYBQFFrO","outputId":"19972bbd-8c75-4167-f115-7d85e8663793","execution":{"iopub.status.busy":"2022-04-14T02:33:48.524951Z","iopub.execute_input":"2022-04-14T02:33:48.526367Z","iopub.status.idle":"2022-04-14T02:33:49.866547Z","shell.execute_reply.started":"2022-04-14T02:33:48.526324Z","shell.execute_reply":"2022-04-14T02:33:49.865881Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"print('average accuracy : ', np.mean(np.array(accuracies)), '+-', np.std(np.array(accuracies)))\nprint('average loss : ', np.mean(np.array(losses)), '+-', np.std(np.array(losses)))\nprint('average jacard : ', np.mean(np.array(jacards)), '+-', np.std(np.array(jacards)))\nprint('average dice_coe : ', np.mean(np.array(dicecoefs)), '+-', np.std(np.array(dicecoefs)))","metadata":{"id":"_EhgaH9kFFrU","outputId":"c6da3f9c-3480-4127-9962-f0522cfcf605","execution":{"iopub.status.busy":"2022-04-14T02:33:49.867678Z","iopub.execute_input":"2022-04-14T02:33:49.868271Z","iopub.status.idle":"2022-04-14T02:33:49.879705Z","shell.execute_reply.started":"2022-04-14T02:33:49.868233Z","shell.execute_reply":"2022-04-14T02:33:49.878903Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"model = load_model('1_unet_ner_seg.hdf5', custom_objects={'dice_coef_loss': dice_coef_loss, 'jacard': jacard, 'dice_coef': dice_coef})","metadata":{"id":"KXFmYOt8SVN1","execution":{"iopub.status.busy":"2022-04-14T02:33:49.881208Z","iopub.execute_input":"2022-04-14T02:33:49.882037Z","iopub.status.idle":"2022-04-14T02:33:51.103176Z","shell.execute_reply.started":"2022-04-14T02:33:49.881995Z","shell.execute_reply":"2022-04-14T02:33:51.102436Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"for i in range(20):\n    index=np.random.randint(0,len(test_data_frame.index))\n    print(i+1, index)\n    img = cv2.imread(test_data_frame['filename'].iloc[index])\n    img = cv2.resize(img, (height, width))\n    img = preprocess_input(img)\n    img = img[np.newaxis, :, :, :]\n    pred = model.predict(img)\n    print(pred[1])\n\n    plt.figure(figsize=(12,12))\n    plt.subplot(1,3,1)\n    plt.imshow(cv2.resize(cv2.imread(test_data_frame['filename'].iloc[index]), (height, width)))\n    plt.title('Original Image')\n    plt.subplot(1,3,2)\n    plt.imshow(np.squeeze(cv2.resize(cv2.imread(test_data_frame['mask'].iloc[index]), (height, width))))\n    plt.title('Original Mask')\n    plt.subplot(1,3,3)\n    plt.imshow(np.squeeze(pred[0]) > .5)\n    plt.title('Prediction')\n    plt.show()","metadata":{"id":"W8HL2A2cFR7h","outputId":"38d748e1-d614-47fc-b4cc-4857f185d8cc","execution":{"iopub.status.busy":"2022-04-14T02:33:51.105057Z","iopub.execute_input":"2022-04-14T02:33:51.105306Z","iopub.status.idle":"2022-04-14T02:34:01.444628Z","shell.execute_reply.started":"2022-04-14T02:33:51.105273Z","shell.execute_reply":"2022-04-14T02:34:01.443679Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"!rm -r generated","metadata":{"execution":{"iopub.status.busy":"2022-04-14T02:34:01.446861Z","iopub.execute_input":"2022-04-14T02:34:01.447831Z","iopub.status.idle":"2022-04-14T02:34:02.483854Z","shell.execute_reply.started":"2022-04-14T02:34:01.447778Z","shell.execute_reply":"2022-04-14T02:34:02.482798Z"},"trusted":true},"execution_count":21,"outputs":[]}]}